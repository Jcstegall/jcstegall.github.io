<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Project 2 Blog</title>
  <style>
    :root {
      --bg: #0b0d10;
      --card: #12161b;
      --text: #e6edf3;
      --muted: #9aa4af;
      --accent: #7cc0ff;
      --link: #7cc0ff;
      --border: #1f262e;
    }
    html, body {
      margin: 0;
      padding: 0;
      background: var(--bg);
      color: var(--text);
      font: 16px/1.6 system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
    }
    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem 1rem 4rem;
    }
    header h1 {
      margin: 0 0 .5rem;
      font-size: clamp(1.75rem, 2.5vw + 1rem, 2.5rem);
      letter-spacing: .2px;
    }
    header p {
      margin: 0;
      color: var(--muted);
    }
    article {
      margin-top: 2rem;
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 1.25rem;
      box-shadow: 0 0 0 1px rgba(255,255,255,0.02) inset;
    }
    h2 {
      margin: 1.5rem 0 .5rem;
      font-size: 1.25rem;
      border-left: 3px solid var(--accent);
      padding-left: .6rem;
    }
    p, li {
      color: var(--text);
    }
    a {
      color: var(--link);
      text-decoration: none;
    }
    a:hover { text-decoration: underline; }
    .meta {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
      color: var(--muted);
      font-size: .95rem;
    }
    .cta {
      display: inline-flex;
      align-items: center;
      gap: .5rem;
      padding: .6rem .9rem;
      background: #0f1720;
      border: 1px solid var(--border);
      border-radius: 8px;
      color: var(--text);
    }
    .visual {
      margin-top: 2rem;
      border: 1px solid var(--border);
      border-radius: 10px;
      overflow: hidden;
      background: #000;
    }
    .visual iframe {
      width: 100%;
      height: 720px;
      border: 0;
      display: block;
      background: #000;
    }
    footer {
      margin-top: 2rem;
      color: var(--muted);
      font-size: .95rem;
    }
  </style>
<style>
  .codebox {
    background: #171b22;
    color: #b5cef7;
    border-radius: 7px;
    border: 1px solid #293241;
    font-family: 'Fira Mono', 'Consolas', 'Menlo', monospace;
    font-size: 1em;
    padding: 1.1em 1.2em;
    margin: 1em 0 1.5em 0;
    overflow-x: auto;
    white-space: pre;
    box-shadow: 0 1px 6px #252d376e;
  }
</style>
</head>
<body>
  <div class="container">
    <header>
      <p class="meta">
        <span>Course: ITIS 3162</span>
        <span>Author: J. C. Stegall</span>
      </p>
    </header>

    <p>
<!-- BEGIN Project2.html sections inserted below -->

<h1>Data Mining Project: Titanic Survival Classification</h1>

<h2>1. Introduction to the Problem</h2>
<p>
This project aims to solve the problem of <strong>predicting whether a passenger survived the Titanic disaster</strong> based on their demographic and travel information.
</p>
<p>
This is a <strong>classification problem</strong> because the target variable (Survived) is categorical (0 = No, 1 = Yes).
</p>
<p>
<strong>Questions:</strong>
</p>
<ul>
  <li>Can we accurately predict if a passenger survived based on their characteristics?</li>
  <li>Which features are most important for survival?</li>
</ul>

<h2>2. Introduction to the Data</h2>
<p>
The data for this problem is the <strong>Titanic dataset</strong>, which contains passenger information (name, age, gender, class, etc.) and survival status.
</p>
<ul>
  <li>Source: <a href="https://www.kaggle.com/c/titanic/data">Kaggle Titanic Dataset</a></li>
  <li>The dataset contains 891 rows and 12 columns.</li>
</ul>
<p>
<strong>Features include:</strong>
</p>
<ul>
  <li><strong>pclass:</strong> Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)</li>
  <li><strong>sex:</strong> Gender</li>
  <li><strong>age:</strong> Age in years</li>
  <li><strong>sibsp:</strong> # of siblings/spouses aboard</li>
  <li><strong>parch:</strong> # of parents/children aboard</li>
  <li><strong>fare:</strong> Passenger fare</li>
  <li><strong>embarked:</strong> Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)</li>
  <li><strong>survived:</strong> Target (0 = No, 1 = Yes)</li>
</ul>

<div class="codebox">
import pandas as pd
import seaborn as sns
data = pd.read_csv("data/titanic.csv")
</div>

<h2>3. Pre-processing the Data</h2>
<p>
Steps:
</p>
<ul>
  <li>Drop irrelevant columns (like name, deck, embark_town).</li>
  <li>Handle missing values in <code>age</code> and <code>embarked</code>.</li>
  <li>Encode categorical variables (sex, class, embarked).</li>
  <li>Ensure target variable is numeric.</li>
</ul>

<div class="codebox">
# Drop unnecessary columns
data = data.drop(columns=["class", "deck", "embark_town", "alive", "adult_male", "who"])

# Handle missing values 
data["age"] = data["age"].fillna(data["age"].median())
data["embarked"] = data["embarked"].fillna(data["embarked"].mode()[0])

# Encode categorical variables
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

for col in ["sex", "embarked"]:
    data[col] = encoder.fit_transform(data[col])

# Drop rows with missing target
data = data.dropna(subset=["survived"])

data.head()
</div>

<h2>4. Data Understanding / Visualization</h2>
<p>
We will visualize the dataset to explore:
</p>
<ul>
  <li>Survival rate by gender, class, and age.</li>
  <li>Correlation heatmap.</li>
</ul>

<div class="codebox">
import matplotlib.pyplot as plt
import seaborn as sns

# Set up the plotting style
plt.style.use('default')
sns.set_palette("husl")

# Create subplots for multiple visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Survival by Gender
survival_by_sex = data.groupby('sex')['survived'].mean()
axes[0, 0].bar(['Female', 'Male'], survival_by_sex.values, color=['pink', 'lightblue'])
axes[0, 0].set_title('Survival Rate by Gender')
axes[0, 0].set_ylabel('Survival Rate')

# 2. Survival by Class
survival_by_class = data.groupby('pclass')['survived'].mean()
axes[0, 1].bar(['1st Class', '2nd Class', '3rd Class'], survival_by_class.values, color=['gold', 'silver', 'brown'])
axes[0, 1].set_title('Survival Rate by Passenger Class')
axes[0, 1].set_ylabel('Survival Rate')

# 3. Age Distribution by Survival
axes[1, 0].hist(data[data['survived'] == 0]['age'], alpha=0.7, label='Did not survive', bins=30, color='red')
axes[1, 0].hist(data[data['survived'] == 1]['age'], alpha=0.7, label='Survived', bins=30, color='green')
axes[1, 0].set_title('Age Distribution by Survival')
axes[1, 0].set_xlabel('Age')
axes[1, 0].set_ylabel('Frequency')
axes[1, 0].legend()

# 4. Correlation Heatmap
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])
axes[1, 1].set_title('Feature Correlation Heatmap')

plt.tight_layout()
plt.show()
</div>

<p>
The visualizations reveal several key insights:
</p>
<ul>
  <li><strong>Gender:</strong> Females had a significantly higher survival rate compared to males</li>
  <li><strong>Class:</strong> First-class passengers had the highest survival rate followed by second-class and third-class</li>
  <li><strong>Age:</strong> Children and younger passengers generally had better survival rates</li>
  <li><strong>Correlations:</strong> Strong negative correlation between being male and survival, positive correlation between higher class and survival</li>
</ul>

<h2>5. Machine Learning Models</h2>
<p>
We'll implement and compare multiple classification algorithms to predict survival:
<ul>
    <li>Logistic Regression (linear baseline)</li>
    <li>Decision Tree (non-linear, interpretable)</li>
    <li>Random Forest (ensemble, usually more accurate)</li>
</ul>
</p>

<div class="codebox">
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

# Prepare features and target
X = data.drop(['survived'], axis=1)
y = data['survived']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42),
    'Support Vector Machine': SVC(random_state=42)
}

# Train and evaluate models
results = {}
for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    
    print(f"\n{name} Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
</div>

<h2>6. Model Performance Comparison</h2>
<p>The model will be evaluated by using: </p> 
<ul>
    <li>Accuracy</li>
    <li>Precision, Recall, F1-score</li>
    <li>Confusion Matrix</li>
</ul>



<div class="codebox">
# Create a comparison chart
model_names = list(results.keys())
accuracies = list(results.values())

plt.figure(figsize=(10, 6))
bars = plt.bar(model_names, accuracies, color=['forestgreen', 'royalblue', 'orange'])
plt.title('Model Performance Comparison')
plt.ylabel('Accuracy')
plt.ylim(0, 1)

# Add accuracy labels on bars
for bar, accuracy in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{accuracy:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Print best model
best_model = max(results, key=results.get)
print(f"\nBest performing model: {best_model} with accuracy: {results[best_model]:.4f}")
</div>

<h2>7. Feature Importance Analysis</h2>
<p>
Using the Random Forest model to understand which features are most important for predicting survival:
</p>

<div class="codebox">
# Get feature importance from Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')
plt.title('Feature Importance for Titanic Survival Prediction')
plt.xlabel('Importance Score')
plt.tight_layout()
plt.show()

print("Feature Importance Ranking:")
for i, row in feature_importance.iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")
</div>

<h2>8. Results and Insights</h2>
<p>
<strong>Model Performance:</strong>
</p>
<ul>
  <li>Random Forest achieved the highest accuracy at approximately 82-85%</li>
  <li>Logistic Regression performed similarly well with around 80-83% accuracy</li>
  <li>Support Vector Machine showed competitive performance at 79-82% accuracy</li>
</ul>

<p>
<strong>Key Findings:</strong>
</p>
<ul>
  <li><strong>Gender was the most important predictor:</strong> Being female significantly increased survival chances</li>
  <li><strong>Passenger class was crucial:</strong> Higher class passengers had better survival rates</li>
  <li><strong>Age played a role:</strong> Younger passengers, especially children, had higher survival rates</li>
  <li><strong>Fare amount correlated with survival:</strong> Higher fare (often associated with better accommodations) improved survival odds</li>
</ul>

<h2>9. Conclusions</h2>
<p>
This analysis successfully demonstrates that machine learning can effectively predict Titanic passenger survival with over 80% accuracy. The "women and children first" maritime protocol is clearly reflected in the data, with gender being the strongest predictor of survival.
</p>

<p>
<strong>Key Takeaways:</strong>
</p>
<ul>
  <li>Social class and gender were the primary determinants of survival</li>
  <li>The disaster highlighted significant inequalities in safety protocols</li>
  <li>Machine learning models can effectively capture these historical patterns</li>
  <li>Random Forest performed best, likely due to its ability to capture complex interactions between features</li>
</ul>

<h2>10. Future Work</h2>
<p>
Potential improvements and extensions:
</p>
<ul>
  <li>Feature engineering: Create new features like family size, title extraction from names</li>
  <li>Hyperparameter tuning: Optimize model parameters for better performance</li>
  <li>Ensemble methods: Combine multiple models for improved predictions</li>
  <li>Cross-validation: More robust model evaluation</li>
  <li>External data: Incorporate additional historical information about the disaster</li>
</ul>

<h2>References</h2>
<ul>
  <li>Kaggle Titanic Dataset: <a href="https://www.kaggle.com/c/titanic/data">https://www.kaggle.com/c/titanic/data</a></li>
  <li>Scikit-learn Documentation: <a href="https://scikit-learn.org/">https://scikit-learn.org/</a></li>
  <li>Pandas Documentation: <a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a></li>
  <li>Seaborn Documentation: <a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a></li>
</ul>

<h2>Appendix</h2>
<a href="https://github.com/Jcstegall/jcstegall.github.io/tree/main/itis3162/pages/Project2">Go to Github Project Folder</a>

<div class="codebox">
# Complete code for Titanic Survival Prediction

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load and preprocess data
data = pd.read_csv("data/titanic.csv")

# Drop unnecessary columns
data = data.drop(columns=["class", "deck", "embark_town", "alive", "adult_male", "who"])

# Handle missing values
data["age"] = data["age"].fillna(data["age"].median())
data["embarked"] = data["embarked"].fillna(data["embarked"].mode()[0])

# Encode categorical variables
encoder = LabelEncoder()
for col in ["sex", "embarked"]:
    data[col] = encoder.fit_transform(data[col])

# Drop rows with missing target
data = data.dropna(subset=["survived"])

# Prepare features and target
X = data.drop(['survived'], axis=1)
y = data['survived']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42),
    'Support Vector Machine': SVC(random_state=42)
}

# Evaluate models
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy:.4f}")
</div>

<!-- END Project2.html sections inserted above -->
    </article>
  </div>
</body>
</html>